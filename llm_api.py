#!/usr/bin/env python3
"""
LLM-Enhanced Drug Search API
Combines vector search with Hugging Face Gemma-2B
"""

import os
import json
import time
import logging
from datetime import datetime
from typing import List, Optional
from pathlib import Path

import chromadb
from chromadb.config import Settings
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# Try importing Hugging Face transformers (optional for demo)
try:
    # Import only if needed, disabled for demo speed
    HAS_TRANSFORMERS = False  # Disabled for demo performance
    logger = logging.getLogger(__name__)
    logger.info("ÔøΩ Using rule-based AI for demo speed")
except ImportError:
    HAS_TRANSFORMERS = False
    logger = logging.getLogger(__name__)
    logger.warning("ÔøΩ Using rule-based responses")

# Try importing OpenAI (fallback)
try:
    import openai
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

# Logging configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global variables
chroma_client = None
collection = None
llm_model = None
llm_tokenizer = None
llm_pipeline = None

class SearchRequest(BaseModel):
    query: str
    limit: int = 10
    minimum_similarity: float = 0.0
    use_llm: bool = True

class SearchResult(BaseModel):
    document_id: str
    document_name: str
    document_type: str
    text_chunk: str
    similarity_score: float
    metadata: dict

class LLMResponse(BaseModel):
    llm_answer: str
    confidence: str
    sources_used: int

class SearchResponse(BaseModel):
    query: str
    results: List[SearchResult]
    llm_response: Optional[LLMResponse] = None
    search_time_ms: int
    total_results: int
    success: bool
    message: str

class HealthResponse(BaseModel):
    status: str
    database_status: str
    total_documents: int
    llm_status: str
    api_version: str
    timestamp: str

# FastAPI app initialization
app = FastAPI(
    title="üß† AI-Powered ƒ∞la√ß Asistanƒ±",
    description="LLM + Vector Search ile Akƒ±llƒ± ƒ∞la√ß Bilgi Sistemi",
    version="2.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def initialize_llm():
    """Initialize rule-based AI for demo speed"""
    global HAS_TRANSFORMERS
    
    logger.info("ü§ñ Initializing fast rule-based AI system...")
    HAS_TRANSFORMERS = True  # Enable rule-based system
    logger.info("‚úÖ Rule-based AI system ready!")
    return True

def initialize_openai_fallback():
    """Initialize OpenAI as fallback if Hugging Face fails"""
    global HAS_OPENAI
    
    if not HAS_OPENAI:
        return False
        
    # Try to get API key from environment
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        logger.warning("ü§ñ OpenAI API key not found - LLM disabled")
        HAS_OPENAI = False
        return False
    
    try:
        openai.api_key = api_key
        # Test API with a simple call
        logger.info("ü§ñ OpenAI LLM initialized successfully")
        return True
    except Exception as e:
        logger.error(f"ü§ñ LLM initialization failed: {e}")
        HAS_OPENAI = False
        return False

def generate_llm_response(query: str, search_results: List[dict]) -> Optional[LLMResponse]:
    """Generate intelligent response using rule-based AI"""
    global HAS_TRANSFORMERS
    
    if not HAS_TRANSFORMERS or not search_results:
        return None
    
    # Use enhanced rule-based response
    return enhanced_rule_based_response(query, search_results)

def enhanced_rule_based_response(query: str, search_results: List[dict]) -> Optional[LLMResponse]:
    """Enhanced rule-based response with smart context extraction"""
    if not search_results:
        return None
    
    top_result = search_results[0]
    text_chunk = top_result.get('text_chunk', '')
    drug_name = top_result.get('metadata', {}).get('drug_name', 'Bu ila√ß')
    
    query_lower = query.lower()
    
    # Advanced pattern matching for Turkish medical queries
    if any(word in query_lower for word in ['yan etki', 'istenmeyen etki', 'zararlƒ±']):
        answer = extract_medical_info(text_chunk, 'yan_etki', drug_name)
    elif any(word in query_lower for word in ['doz', 'miktar', 'ka√ß tane', 'ne kadar']):
        answer = extract_medical_info(text_chunk, 'doz', drug_name)
    elif any(word in query_lower for word in ['nasƒ±l kullan', 'nasƒ±l al', 'kullanƒ±m ≈üekli']):
        answer = extract_medical_info(text_chunk, 'kullanim', drug_name)
    elif any(word in query_lower for word in ['nedir', 'ne i√ßin', 'hangi hastalƒ±k']):
        answer = extract_medical_info(text_chunk, 'genel', drug_name)
    elif any(word in query_lower for word in ['kimler kullanmamalƒ±', 'kontrendikasyon', 'yasak']):
        answer = extract_medical_info(text_chunk, 'kontrendikasyon', drug_name)
    elif any(word in query_lower for word in ['hamilelik', 'gebelik', 'emzirme']):
        answer = extract_medical_info(text_chunk, 'hamilelik', drug_name)
    else:
        # General information
        answer = f"{drug_name} hakkƒ±nda bilgi: {text_chunk[:200]}..."
    
    return LLMResponse(
        llm_answer=answer,
        confidence="Akƒ±llƒ± Analiz",
        sources_used=len(search_results)
    )

def extract_medical_info(text: str, info_type: str, drug_name: str) -> str:
    """Extract specific medical information from text"""
    sentences = text.split('.')
    
    keywords = {
        'yan_etki': ['yan etki', 'istenmeyen etki', 'reaksiyon', 'zararlƒ± etki'],
        'doz': ['doz', 'miktar', 'g√ºnde', 'tablet', 'mg', 'ml', 'ka√ß'],
        'kullanim': ['kullanƒ±m', 'alƒ±nƒ±r', 'nasƒ±l', '≈üekli', 'y√∂ntemi'],
        'genel': ['etken madde', 'i√ßerik', 'nedir', 'tedavi', 'hastalƒ±k'],
        'kontrendikasyon': ['kullanmamalƒ±', 'yasak', 'sakƒ±ncalƒ±', 'kontrendikasyon'],
        'hamilelik': ['hamilelik', 'gebelik', 'emzirme', 'anne']
    }
    
    relevant_sentences = []
    for sentence in sentences:
        if any(keyword in sentence.lower() for keyword in keywords.get(info_type, [])):
            relevant_sentences.append(sentence.strip())
    
    if relevant_sentences:
        result = '. '.join(relevant_sentences[:2])
        return f"{drug_name} - {result}."
    else:
        # Fallback to first meaningful sentence
        meaningful_sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        if meaningful_sentences:
            return f"{drug_name} hakkƒ±nda: {meaningful_sentences[0]}."
        else:
            return f"{drug_name} hakkƒ±nda bilgi prospekt√ºs i√ßeriƒüinde mevcuttur."

def generate_openai_response(query: str, search_results: List[dict]) -> Optional[LLMResponse]:
    """Generate response using OpenAI as fallback"""
    if not HAS_OPENAI or not search_results:
        return None
    
    try:
        # Prepare context from search results
        context = "ƒ∞la√ß bilgileri:\n"
        for i, result in enumerate(search_results[:3]):  # Use top 3 results
            drug_name = result.get('metadata', {}).get('drug_name', 'Bilinmeyen')
            text = result.get('text_chunk', '')[:200]  # Limit text length
            context += f"{i+1}. {drug_name}: {text}...\n"
        
        # Create prompt
        prompt = f"""Sen bir uzman eczacƒ±sƒ±n. A≈üaƒüƒ±daki soruya g√∂re ila√ß bilgilerini kullanarak yardƒ±mcƒ± ol:

Soru: {query}

Mevcut ƒ∞la√ß Bilgileri:
{context}

L√ºtfen:
1. Soruya √∂zel, faydalƒ± bir cevap ver
2. Mevcut bilgilere dayanarak konu≈ü
3. Kesin te≈ühis koymaktan ka√ßƒ±n
4. Doktora danƒ±≈ümasƒ±nƒ± √∂ner
5. T√ºrk√ße ve anla≈üƒ±lƒ±r bir dil kullan

Cevap:"""

        # Call OpenAI API
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Sen uzman bir eczacƒ±sƒ±n ve ila√ß konularƒ±nda g√ºvenilir bilgi veriyorsun."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=300,
            temperature=0.7
        )
        
        llm_answer = response.choices[0].message.content.strip()
        
        return LLMResponse(
            llm_answer=llm_answer,
            confidence="medium",
            sources_used=len(search_results[:3])
        )
        
    except Exception as e:
        logger.error(f"ü§ñ LLM response generation failed: {e}")
        return None

def initialize_database():
    """ChromaDB connection initialize et"""
    global chroma_client, collection
    
    try:
        logger.info("üîå ChromaDB baƒülantƒ±sƒ± kuruluyor...")
        
        # ChromaDB client - yeni veritabanƒ± yolu
        chroma_client = chromadb.PersistentClient(
            path="./data/veritabani_optimized",
            settings=Settings(allow_reset=True)
        )
        
        # Collection baƒülantƒ±sƒ± - yeni collection adƒ±
        collection_name = "ilac_prospektusleri"
        try:
            collection = chroma_client.get_collection(collection_name)
            doc_count = collection.count()
            logger.info(f"‚úÖ Collection bulundu: {doc_count:,} documents")
        except Exception as e:
            logger.error(f"‚ùå Collection bulunamadƒ±: {e}")
            raise HTTPException(status_code=500, detail="Vector database bulunamadƒ±")
            
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Database initialization hatasƒ±: {e}")
        return False

@app.on_event("startup")
async def startup_event():
    """API ba≈ülangƒ±√ß i≈ülemleri"""
    logger.info("üöÄ AI-Powered ProspektAsistan API ba≈ülatƒ±lƒ±yor...")
    
    # Initialize database
    db_success = initialize_database()
    if not db_success:
        logger.error("‚ùå Database initialization ba≈üarƒ±sƒ±z!")
        return
    
    # Try to initialize rule-based AI system
    llm_success = initialize_llm()
    if llm_success:
        logger.info("ü§ñ Rule-based AI sistemi aktif")
    else:
        # Fallback to OpenAI
        openai_success = initialize_openai_fallback()
        if openai_success:
            logger.info("ü§ñ OpenAI LLM fallback aktif")
        else:
            logger.info("üîç Sadece vector search aktif")
    
    logger.info("‚úÖ API ba≈üarƒ±yla ba≈ülatƒ±ldƒ±!")

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """API ve database durumu"""
    try:
        doc_count = collection.count() if collection else 0
        
        # Determine LLM status
        llm_status = "unavailable"
        if HAS_TRANSFORMERS:
            llm_status = "rule-based-ai"
        elif HAS_OPENAI:
            llm_status = "openai"
        
        return HealthResponse(
            status="healthy",
            database_status="connected" if collection else "disconnected",
            total_documents=doc_count,
            llm_status=llm_status,
            api_version="2.0.0",
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"Health check hatasƒ±: {e}")
        return HealthResponse(
            status="unhealthy",
            database_status="error",
            total_documents=0,
            llm_status="error",
            api_version="2.0.0",
            timestamp=datetime.now().isoformat()
        )

@app.post("/search", response_model=SearchResponse)
async def enhanced_search(request: SearchRequest):
    """
    AI-Enhanced ƒ∞la√ß Arama
    - Vector search + LLM intelligence
    - Akƒ±llƒ± cevaplar ve √∂neriler
    """
    start_time = time.time()
    
    try:
        if collection is None:
            raise HTTPException(status_code=500, detail="Database baƒülantƒ±sƒ± yok")
        
        # Input validation
        if not request.query.strip():
            raise HTTPException(status_code=400, detail="Query bo≈ü olamaz")
        
        logger.info(f"üîç Enhanced search: '{request.query}' (LLM: {request.use_llm})")
        
        # Vector search
        results = collection.query(
            query_texts=[request.query],
            n_results=min(request.limit, 20),  # Limit for performance
            include=['documents', 'metadatas', 'distances']
        )
        
        # Format results
        formatted_results = []
        if results['documents'][0]:
            documents = results['documents'][0]
            metadatas = results['metadatas'][0]
            distances = results['distances'][0]
            ids = results['ids'][0]
            
            for i, (doc, metadata, distance, doc_id) in enumerate(zip(documents, metadatas, distances, ids)):
                # Better similarity calculation
                similarity = max(0, 1 - (distance / 2))  # Normalize distance better
                
                if similarity >= request.minimum_similarity:
                    result = SearchResult(
                        document_id=doc_id,
                        document_name=f"Document_{i+1}",
                        document_type=metadata.get('document_type', 'KUB'),
                        text_chunk=doc.strip()[:300] + "..." if len(doc) > 300 else doc.strip(),
                        similarity_score=round(similarity, 3),
                        metadata=metadata
                    )
                    formatted_results.append(result)
        
        # Generate LLM response if requested and available
        llm_response = None
        if request.use_llm and formatted_results:
            # Try Hugging Face first, then OpenAI fallback
            if HAS_TRANSFORMERS and llm_pipeline:
                llm_response = generate_llm_response(request.query, [r.dict() for r in formatted_results])
            elif HAS_OPENAI:
                llm_response = generate_openai_response(request.query, [r.dict() for r in formatted_results])
        
        search_time = time.time() - start_time
        
        return SearchResponse(
            query=request.query,
            results=formatted_results[:request.limit],
            llm_response=llm_response,
            search_time_ms=int(search_time * 1000),
            total_results=len(formatted_results),
            success=True,
            message=f"{len(formatted_results)} sonu√ß bulundu" + (" + AI analizi" if llm_response else "")
        )
        
    except Exception as e:
        logger.error(f"‚ùå Search error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
async def root():
    """API ana sayfa"""
    return {
        "message": "üß† AI-Powered ƒ∞la√ß Asistanƒ± API",
        "version": "2.0.0",
        "features": ["Vector Search", "LLM Intelligence", "Real-time Analysis"],
        "docs": "/docs",
        "health": "/health"
    }

if __name__ == "__main__":
    import os
    port = int(os.getenv("PORT", 8003))  # Railway PORT veya local 8003
    uvicorn.run(
        "llm_api:app",
        host="0.0.0.0",
        port=port,
        reload=False
    )