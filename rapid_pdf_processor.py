#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
‚ö° S√úRAHIYEN HIZINDA PDF PROCESSING! ‚ö°
5,275 PDF'i paralel olarak text extraction + ChromaDB loading
"""

import os
import sys
import time
import logging
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from multiprocessing import cpu_count
import chromadb
import hashlib
import json
from typing import List, Dict, Optional, Tuple
import traceback

# PDF processing libraries
try:
    import PyPDF2
    import pdfplumber
    from sentence_transformers import SentenceTransformer
except ImportError as e:
    print(f"‚ùå Eksik k√ºt√ºphane: {e}")
    print("üì¶ Kurulum: pip install PyPDF2 pdfplumber sentence-transformers")
    sys.exit(1)

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('processing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RapidPDFProcessor:
    def __init__(self):
        self.base_dir = Path("C:/pupillicaHackathon")
        self.kt_dir = self.base_dir / "data/kt"
        self.kub_dir = self.base_dir / "data/kub"
        self.processed_dir = self.base_dir / "data/processed/extracted"
        self.vector_db_path = self.base_dir / "data/chroma_db"
        
        # Create directories
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        
        # Stats
        self.total_files = 0
        self.processed_files = 0
        self.failed_files = 0
        self.start_time = time.time()
        
        # Process tracking
        self.processed_hashes = set()
        self.load_processed_hashes()
        
    def load_processed_hashes(self):
        """Daha √∂nce process edilmi≈ü dosyalarƒ± y√ºkle"""
        hash_file = self.processed_dir / "processed_hashes.json"
        if hash_file.exists():
            try:
                with open(hash_file, 'r') as f:
                    data = json.load(f)
                    self.processed_hashes = set(data.get('hashes', []))
                logger.info(f"üìã {len(self.processed_hashes)} dosya hash'i y√ºklendi")
            except Exception as e:
                logger.warning(f"Hash dosyasƒ± y√ºklenemedi: {e}")
    
    def save_processed_hash(self, file_hash: str):
        """Process edilmi≈ü dosya hash'ini kaydet"""
        self.processed_hashes.add(file_hash)
        hash_file = self.processed_dir / "processed_hashes.json"
        try:
            with open(hash_file, 'w') as f:
                json.dump({'hashes': list(self.processed_hashes)}, f)
        except Exception as e:
            logger.warning(f"Hash kayƒ±t hatasƒ±: {e}")
    
    def get_file_hash(self, file_path: Path) -> str:
        """Dosya hash'i hesapla"""
        hasher = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                # ƒ∞lk 8KB'ƒ± hash'le (hƒ±z i√ßin)
                chunk = f.read(8192)
                hasher.update(chunk)
                hasher.update(str(file_path.stat().st_size).encode())
        except:
            return None
        return hasher.hexdigest()
    
    def extract_text_from_pdf(self, pdf_path: Path) -> Optional[Dict]:
        """PDF'den text extract et"""
        try:
            # Hash kontrol√º
            file_hash = self.get_file_hash(pdf_path)
            if file_hash in self.processed_hashes:
                return None  # Daha √∂nce process edilmi≈ü
            
            text_content = ""
            metadata = {
                "file_name": pdf_path.name,
                "file_path": str(pdf_path),
                "file_size": pdf_path.stat().st_size,
                "file_hash": file_hash,
                "source_type": "kt" if "/kt/" in str(pdf_path) else "kub",
                "extraction_method": "pdfplumber"
            }
            
            # PDFPlumber ile deneme (daha kaliteli)
            try:
                with pdfplumber.open(pdf_path) as pdf:
                    for page_num, page in enumerate(pdf.pages[:50]):  # ƒ∞lk 50 sayfa (hƒ±z)
                        try:
                            page_text = page.extract_text()
                            if page_text:
                                text_content += f"\\n\\n--- Sayfa {page_num + 1} ---\\n{page_text}"
                        except:
                            continue
                            
                if len(text_content) < 100:  # √áok az text varsa PyPDF2 dene
                    raise Exception("Insufficient text from pdfplumber")
                    
                metadata["extraction_method"] = "pdfplumber"
                
            except Exception as e:
                # PyPDF2 ile fallback
                try:
                    with open(pdf_path, 'rb') as file:
                        pdf_reader = PyPDF2.PdfReader(file)
                        for page_num in range(min(len(pdf_reader.pages), 50)):  # ƒ∞lk 50 sayfa
                            try:
                                page = pdf_reader.pages[page_num]
                                page_text = page.extract_text()
                                if page_text:
                                    text_content += f"\\n\\n--- Sayfa {page_num + 1} ---\\n{page_text}"
                            except:
                                continue
                    metadata["extraction_method"] = "PyPDF2"
                    
                except Exception as e2:
                    logger.error(f"‚ùå PDF extraction failed for {pdf_path.name}: {e2}")
                    return None
            
            # Text temizleme
            text_content = text_content.strip()
            if len(text_content) < 50:
                logger.warning(f"‚ö†Ô∏è Az text: {pdf_path.name} - {len(text_content)} karakter")
                return None
            
            # Metadata g√ºncelleme
            metadata["text_length"] = len(text_content)
            metadata["page_count"] = text_content.count("--- Sayfa")
            
            # Hash'i kaydet
            self.save_processed_hash(file_hash)
            
            return {
                "text": text_content,
                "metadata": metadata
            }
            
        except Exception as e:
            logger.error(f"‚ùå PDF processing hatasƒ± {pdf_path.name}: {e}")
            return None
    
    def get_pdf_files(self) -> List[Path]:
        """T√ºm PDF dosyalarƒ±nƒ± bul"""
        pdf_files = []
        
        # KT klas√∂r√º
        if self.kt_dir.exists():
            kt_files = list(self.kt_dir.glob("*.pdf"))
            pdf_files.extend(kt_files)
            logger.info(f"üìÅ KT klas√∂r√º: {len(kt_files)} PDF")
        
        # KUB klas√∂r√º  
        if self.kub_dir.exists():
            kub_files = list(self.kub_dir.glob("*.pdf"))
            pdf_files.extend(kub_files)
            logger.info(f"üìÅ KUB klas√∂r√º: {len(kub_files)} PDF")
        
        self.total_files = len(pdf_files)
        logger.info(f"üéØ Toplam: {self.total_files} PDF dosyasƒ± bulundu")
        
        return pdf_files
    
    def process_batch(self, pdf_files: List[Path], batch_size: int = 100) -> List[Dict]:
        """PDF'leri batch halinde process et"""
        results = []
        
        # CPU sayƒ±sƒ±na g√∂re worker sayƒ±sƒ±
        max_workers = min(cpu_count(), 8)  # Max 8 worker
        logger.info(f"üöÄ {max_workers} parallel worker ile processing ba≈ülƒ±yor...")
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # Submit batch
            future_to_file = {
                executor.submit(self.extract_text_from_pdf, pdf_file): pdf_file 
                for pdf_file in pdf_files[:batch_size]
            }
            
            # Collect results
            for future in as_completed(future_to_file):
                pdf_file = future_to_file[future]
                try:
                    result = future.result(timeout=30)  # 30 saniye timeout
                    if result:
                        results.append(result)
                        self.processed_files += 1
                    else:
                        self.failed_files += 1
                        
                    # Progress raporu
                    if (self.processed_files + self.failed_files) % 50 == 0:
                        elapsed = time.time() - self.start_time
                        rate = (self.processed_files + self.failed_files) / elapsed
                        eta = (self.total_files - self.processed_files - self.failed_files) / rate
                        
                        logger.info(f"‚ö° Progress: {self.processed_files}/{self.total_files} "
                                  f"({self.processed_files/self.total_files*100:.1f}%) "
                                  f"- Rate: {rate:.1f} files/sec - ETA: {eta/60:.1f} min")
                        
                except Exception as e:
                    logger.error(f"‚ùå Worker error for {pdf_file.name}: {e}")
                    self.failed_files += 1
        
        return results
    
    def save_extracted_texts(self, results: List[Dict]):
        """Extract edilmi≈ü textleri kaydet"""
        for i, result in enumerate(results):
            try:
                # Dosya adƒ±
                file_name = f"extracted_{i:06d}_{result['metadata']['file_hash'][:8]}.json"
                output_path = self.processed_dir / file_name
                
                # JSON olarak kaydet
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                    
            except Exception as e:
                logger.error(f"‚ùå Text kayƒ±t hatasƒ±: {e}")
    
    def run_rapid_processing(self, batch_size: int = 500):
        """Hƒ±zlƒ± processing'i ba≈ülat"""
        logger.info("üöÄ S√úRAHƒ∞YA HIZINDA PDF PROCESSING BA≈ûLIYOR!")
        
        # PDF dosyalarƒ±nƒ± bul
        pdf_files = self.get_pdf_files()
        if not pdf_files:
            logger.error("‚ùå PDF dosyasƒ± bulunamadƒ±!")
            return False
        
        # Batch'ler halinde process et
        all_results = []
        for i in range(0, len(pdf_files), batch_size):
            batch = pdf_files[i:i+batch_size]
            logger.info(f"üì¶ Batch {i//batch_size + 1}: {len(batch)} dosya")
            
            batch_results = self.process_batch(batch, len(batch))
            all_results.extend(batch_results)
            
            # Intermediate save
            if batch_results:
                self.save_extracted_texts(batch_results)
                
            logger.info(f"‚úÖ Batch tamamlandƒ±: {len(batch_results)} ba≈üarƒ±lƒ±")
        
        # Final sonu√ßlar
        elapsed = time.time() - self.start_time
        success_rate = (self.processed_files / self.total_files) * 100
        
        logger.info(f"")
        logger.info(f"üéØ PROCESSING TAMAMLANDI!")
        logger.info(f"üìä Toplam dosya: {self.total_files}")
        logger.info(f"‚úÖ Ba≈üarƒ±lƒ±: {self.processed_files} ({success_rate:.1f}%)")
        logger.info(f"‚ùå Ba≈üarƒ±sƒ±z: {self.failed_files}")
        logger.info(f"‚è±Ô∏è S√ºre: {elapsed/60:.1f} dakika")
        logger.info(f"‚ö° Ortalama hƒ±z: {self.total_files/elapsed:.1f} dosya/saniye")
        logger.info(f"üíæ Extracted files: {len(list(self.processed_dir.glob('*.json')))}")
        
        return len(all_results) > 0

if __name__ == "__main__":
    processor = RapidPDFProcessor()
    success = processor.run_rapid_processing(batch_size=200)
    
    if success:
        print("üéâ PDF Processing tamamlandƒ±! ChromaDB loading'e ge√ßiliyor...")
    else:
        print("‚ùå PDF Processing ba≈üarƒ±sƒ±z!")
        sys.exit(1)