#!/usr/bin/env python3
"""
PUPILLICA - FULL SCALE PDF EXTRACTION (Optimized)
TÃ¼m 5,275 PDF'i iÅŸleyip text extraction yapmak (seri processing)
"""

import os
import json
from pathlib import Path
from pdf_extraction import PDFTextExtractor
from tqdm import tqdm
from datetime import datetime
import time

# Dizinler
DATA_DIR = Path("data")
KT_DIR = DATA_DIR / "kt"
KUB_DIR = DATA_DIR / "kub"
PROCESSED_DIR = DATA_DIR / "processed"
EXTRACTED_DIR = PROCESSED_DIR / "extracted"

def extract_single_pdf(pdf_path):
    """Tek PDF'i iÅŸle"""
    
    try:
        extractor = PDFTextExtractor()
        text, method, results = extractor.extract_text(pdf_path)
        
        if text and len(text.strip()) > 50:
            # Dosya adÄ±ndan ilaÃ§ ismini Ã§Ä±kar
            file_name = Path(pdf_path).stem
            if '_KT' in file_name:
                drug_name = file_name.replace('_KT', '')
            elif '_KUB' in file_name:
                drug_name = file_name.replace('_KUB', '')
            else:
                drug_name = file_name
            
            return {
                'file_path': str(pdf_path),
                'drug_name': drug_name,
                'text': text,
                'text_length': len(text),
                'extraction_method': method,
                'success': True
            }
        else:
            return {
                'file_path': str(pdf_path),
                'success': False,
                'error': f'No text extracted or text too short (method: {method})'
            }
            
    except Exception as e:
        return {
            'file_path': str(pdf_path),
            'success': False,
            'error': str(e)
        }

def process_document_type(doc_files, doc_type, batch_size=500):
    """Bir dokÃ¼man tipini iÅŸle (KT veya KUB)"""
    
    print(f"ğŸ”„ {doc_type} Ä°ÅLENÄ°YOR - {len(doc_files)} dosya")
    
    all_results = []
    total_batches = (len(doc_files) + batch_size - 1) // batch_size
    
    for i in range(0, len(doc_files), batch_size):
        batch = doc_files[i:i+batch_size]
        batch_num = i // batch_size + 1
        
        print(f"ğŸ“¦ Batch {batch_num}/{total_batches} - {len(batch)} dosya")
        
        batch_results = []
        
        # Batch'i iÅŸle
        for pdf_file in tqdm(batch, desc=f"{doc_type} Batch {batch_num}"):
            result = extract_single_pdf(pdf_file)
            batch_results.append(result)
        
        # Batch sonuÃ§larÄ±nÄ± kaydet
        save_batch_results(batch_results, doc_type, batch_num)
        all_results.extend(batch_results)
        
        # Ä°statistik gÃ¶ster
        successful = len([r for r in batch_results if r['success']])
        print(f"  âœ… Batch {batch_num}: {successful}/{len(batch_results)} baÅŸarÄ±lÄ±")
        
        # Memory temizle
        del batch_results
    
    return all_results

def save_batch_results(results, doc_type, batch_num):
    """Batch sonuÃ§larÄ±nÄ± kaydet"""
    
    batch_dir = EXTRACTED_DIR / "full_batches" / doc_type.lower()
    batch_dir.mkdir(parents=True, exist_ok=True)
    
    # BaÅŸarÄ±lÄ± extractions
    successful = [r for r in results if r['success']]
    if successful:
        batch_file = batch_dir / f"batch_{batch_num:03d}.json"
        with open(batch_file, 'w', encoding='utf-8') as f:
            json.dump(successful, f, ensure_ascii=False, indent=2)
    
    # Error log
    errors = [r for r in results if not r['success']]
    if errors:
        error_file = batch_dir / f"errors_batch_{batch_num:03d}.json"
        with open(error_file, 'w', encoding='utf-8') as f:
            json.dump(errors, f, ensure_ascii=False, indent=2)

def merge_all_results(doc_type):
    """TÃ¼m batch sonuÃ§larÄ±nÄ± birleÅŸtir"""
    
    batch_dir = EXTRACTED_DIR / "full_batches" / doc_type.lower()
    all_results = []
    
    # TÃ¼m batch dosyalarÄ±nÄ± oku
    for batch_file in sorted(batch_dir.glob("batch_*.json")):
        with open(batch_file, 'r', encoding='utf-8') as f:
            batch_data = json.load(f)
            all_results.extend(batch_data)
    
    # BirleÅŸtirilmiÅŸ dosyayÄ± kaydet
    output_file = EXTRACTED_DIR / f"{doc_type.lower()}_extracted_full.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ {doc_type} birleÅŸtirilmiÅŸ: {len(all_results)} baÅŸarÄ±lÄ± extraction")
    return all_results

def create_summary_report(kt_results, kub_results):
    """Ã–zet rapor oluÅŸtur"""
    
    summary = {
        "timestamp": datetime.now().isoformat(),
        "total_files_processed": len(kt_results) + len(kub_results),
        "kt_summary": {
            "total_files": len(kt_results),
            "successful_extractions": len([r for r in kt_results if r.get('success', False)]),
            "success_rate": len([r for r in kt_results if r.get('success', False)]) / len(kt_results) if kt_results else 0,
            "average_text_length": sum(r.get('text_length', 0) for r in kt_results if r.get('success', False)) / len([r for r in kt_results if r.get('success', False)]) if [r for r in kt_results if r.get('success', False)] else 0
        },
        "kub_summary": {
            "total_files": len(kub_results),
            "successful_extractions": len([r for r in kub_results if r.get('success', False)]),
            "success_rate": len([r for r in kub_results if r.get('success', False)]) / len(kub_results) if kub_results else 0,
            "average_text_length": sum(r.get('text_length', 0) for r in kub_results if r.get('success', False)) / len([r for r in kub_results if r.get('success', False)]) if [r for r in kub_results if r.get('success', False)] else 0
        }
    }
    
    kt_successful = [r for r in kt_results if r.get('success', False)]
    kub_successful = [r for r in kub_results if r.get('success', False)]
    
    summary["overall"] = {
        "total_successful": len(kt_successful) + len(kub_successful),
        "overall_success_rate": (len(kt_successful) + len(kub_successful)) / (len(kt_results) + len(kub_results)),
        "total_text_extracted": sum(r.get('text_length', 0) for r in kt_successful + kub_successful)
    }
    
    # Raporu kaydet
    summary_file = EXTRACTED_DIR / "extraction_summary_full.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    return summary

def main():
    """Ana iÅŸleme fonksiyonu"""
    
    print("ğŸš€ PUPILLICA - FULL SCALE PDF EXTRACTION (OPTIMIZED)")
    print("=" * 70)
    
    start_time = time.time()
    
    # Dizinleri oluÅŸtur
    EXTRACTED_DIR.mkdir(parents=True, exist_ok=True)
    
    # TÃ¼m PDF'leri listele
    kt_files = list(KT_DIR.rglob("*.pdf"))
    kub_files = list(KUB_DIR.rglob("*.pdf"))
    
    print(f"ğŸ“ KT PDFs: {len(kt_files)}")
    print(f"ğŸ“ KUB PDFs: {len(kub_files)}")
    print(f"ğŸ“ TOPLAM: {len(kt_files) + len(kub_files)}")
    
    if not kt_files and not kub_files:
        print("âŒ HiÃ§ PDF dosyasÄ± bulunamadÄ±!")
        return
    
    # KT dosyalarÄ±nÄ± iÅŸle
    print(f"\n" + "="*50)
    kt_results = process_document_type(kt_files, "KT")
    kt_merged = merge_all_results("KT")
    
    # KUB dosyalarÄ±nÄ± iÅŸle
    print(f"\n" + "="*50)
    kub_results = process_document_type(kub_files, "KUB")
    kub_merged = merge_all_results("KUB")
    
    # Ã–zet rapor
    print(f"\n" + "="*50)
    summary = create_summary_report(kt_results, kub_results)
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"\nğŸ“Š FINAL Ã–ZET:")
    print(f"  Toplam dosya: {summary['total_files_processed']}")
    print(f"  KT baÅŸarÄ±lÄ±: {summary['kt_summary']['successful_extractions']}/{summary['kt_summary']['total_files']} ({summary['kt_summary']['success_rate']:.1%})")
    print(f"  KUB baÅŸarÄ±lÄ±: {summary['kub_summary']['successful_extractions']}/{summary['kub_summary']['total_files']} ({summary['kub_summary']['success_rate']:.1%})")
    print(f"  Toplam baÅŸarÄ±lÄ±: {summary['overall']['total_successful']}")
    print(f"  Genel baÅŸarÄ± oranÄ±: {summary['overall']['overall_success_rate']:.1%}")
    print(f"  Toplam metin: {summary['overall']['total_text_extracted']:,} karakter")
    print(f"  Ä°ÅŸlem sÃ¼resi: {processing_time/60:.1f} dakika")
    
    print(f"\nğŸ¯ SONRAKÄ° ADIM: Vector database'e indexleme")
    print(f"  Tahmini chunk sayÄ±sÄ±: ~{summary['overall']['total_successful'] * 40:,}")
    
    return summary

if __name__ == "__main__":
    summary = main()