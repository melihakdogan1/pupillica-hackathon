#!/usr/bin/env python3
"""
PUPILLICA - FULL SCALE VECTOR DATABASE
4,816 PDF'den Ã§Ä±karÄ±lan metinleri ChromaDB'ye indexleme
"""

import os
import json
from pathlib import Path
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List, Dict, Tuple
from datetime import datetime
import hashlib
from tqdm import tqdm
import re
import time

# Dizinler
DATA_DIR = Path("data")
PROCESSED_DIR = DATA_DIR / "processed"
EXTRACTED_DIR = PROCESSED_DIR / "extracted"
VECTOR_DB_DIR = PROCESSED_DIR / "vector_db_full"

class PupillicaFullVectorDB:
    """Pupillica Full Scale Vector Database YÃ¶netim SÄ±nÄ±fÄ±"""
    
    def __init__(self, db_path: Path = None, embedding_model: str = "all-MiniLM-L6-v2"):
        """
        Vector DB'yi baÅŸlat
        """
        self.db_path = db_path or VECTOR_DB_DIR
        self.embedding_model_name = embedding_model
        
        # ChromaDB ayarlarÄ±
        os.makedirs(self.db_path, exist_ok=True)
        self.client = chromadb.PersistentClient(
            path=str(self.db_path),
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Embedding model yÃ¼kle
        print(f"ğŸ¤– Embedding model yÃ¼kleniyor: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        print(f"âœ… Model hazÄ±r - Boyut: {self.embedding_model.get_sentence_embedding_dimension()}")
        
        # Collection oluÅŸtur
        self.collection = self.client.get_or_create_collection(
            name="pupillica_full",
            metadata={"description": "Pupillica Full Scale Drug Information Database"}
        )
    
    def load_extracted_data(self):
        """Ã‡Ä±karÄ±lan PDF verilerini yÃ¼kle"""
        
        print("ğŸ“š Extracted veriler yÃ¼kleniyor...")
        
        # KT ve KUB dosyalarÄ±nÄ± yÃ¼kle
        kt_file = EXTRACTED_DIR / "kt_extracted_parallel.json"
        kub_file = EXTRACTED_DIR / "kub_extracted_parallel.json"
        
        kt_data = []
        kub_data = []
        
        if kt_file.exists():
            with open(kt_file, 'r', encoding='utf-8') as f:
                kt_data = json.load(f)
            print(f"ğŸ“‹ KT verisi: {len(kt_data)} dosya")
        
        if kub_file.exists():
            with open(kub_file, 'r', encoding='utf-8') as f:
                kub_data = json.load(f)
            print(f"ğŸ“‹ KUB verisi: {len(kub_data)} dosya")
        
        total_data = kt_data + kub_data
        print(f"ğŸ“Š Toplam: {len(total_data)} dosya yÃ¼klendi")
        
        return total_data
    
    def clean_text(self, text: str) -> str:
        """Metni temizle"""
        if not text:
            return ""
        
        # Ã‡ok fazla boÅŸluk ve satÄ±r sonu temizle
        text = re.sub(r'\s+', ' ', text)
        
        # BaÅŸÄ±ndaki ve sonundaki boÅŸluklarÄ± temizle
        text = text.strip()
        
        return text
    
    def create_chunks(self, text: str, chunk_size: int = 300, overlap: int = 30) -> List[str]:
        """Metni chunk'lara bÃ¶l - optimize edilmiÅŸ"""
        
        if not text or len(text) < chunk_size:
            return [text] if text else []
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # Son chunk'sa, metni bitir
            if end >= len(text):
                chunks.append(text[start:])
                break
            
            # Kelime sÄ±nÄ±rÄ±nda kes
            chunk = text[start:end]
            last_space = chunk.rfind(' ')
            
            if last_space > chunk_size * 0.7:  # En az %70'inde boÅŸluk varsa
                chunk = chunk[:last_space]
                end = start + last_space
            
            chunks.append(chunk)
            start = end - overlap
        
        return [chunk.strip() for chunk in chunks if chunk.strip()]
    
    def process_document(self, doc_data: dict) -> List[dict]:
        """Tek dokÃ¼manÄ± iÅŸle ve chunk'larÄ± oluÅŸtur"""
        
        file_path = doc_data['file_path']
        drug_name = doc_data['drug_name']
        text = doc_data['text']
        extraction_method = doc_data['extraction_method']
        
        # Metin tipini belirle
        doc_type = "KT" if "_KT" in file_path else "KUB"
        
        # Metni temizle
        cleaned_text = self.clean_text(text)
        
        if not cleaned_text:
            return []
        
        # Chunk'lara bÃ¶l
        chunks = self.create_chunks(cleaned_text)
        
        # Her chunk iÃ§in metadata oluÅŸtur
        processed_chunks = []
        for i, chunk in enumerate(chunks):
            chunk_id = f"{drug_name}_{doc_type}_{i:03d}"
            
            chunk_data = {
                'chunk_id': chunk_id,
                'text': chunk,
                'metadata': {
                    'drug_name': drug_name,
                    'document_type': doc_type,
                    'chunk_index': i,
                    'total_chunks': len(chunks),
                    'file_path': file_path,
                    'extraction_method': extraction_method,
                    'text_length': len(chunk),
                    'source_doc_length': len(cleaned_text)
                }
            }
            processed_chunks.append(chunk_data)
        
        return processed_chunks
    
    def batch_embed_and_store(self, chunks: List[dict], batch_size: int = 100):
        """Chunk'larÄ± batch'ler halinde embed edip kaydet"""
        
        print(f"ğŸ”„ {len(chunks)} chunk embedding ve kayÄ±t iÅŸlemi...")
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]
            
            # Tekstleri ve metadata'larÄ± ayÄ±r
            texts = [chunk['text'] for chunk in batch]
            ids = [chunk['chunk_id'] for chunk in batch]
            metadatas = [chunk['metadata'] for chunk in batch]
            
            # Embeddings oluÅŸtur
            embeddings = self.embedding_model.encode(texts, show_progress_bar=False)
            
            # ChromaDB'ye kaydet
            self.collection.add(
                ids=ids,
                embeddings=embeddings.tolist(),
                documents=texts,
                metadatas=metadatas
            )
            
            print(f"  âœ… Batch {i//batch_size + 1}: {len(batch)} chunk kaydedildi")
    
    def create_full_vector_database(self):
        """Full scale vector database oluÅŸtur"""
        
        print("ğŸš€ PUPILLICA FULL SCALE VECTOR DATABASE OLUÅTURULUYOR")
        print("=" * 70)
        
        start_time = time.time()
        
        # Veriyi yÃ¼kle
        documents = self.load_extracted_data()
        
        if not documents:
            print("âŒ YÃ¼klenecek veri bulunamadÄ±!")
            return
        
        # DokÃ¼manlarÄ± iÅŸle
        print(f"\nğŸ“ {len(documents)} dokÃ¼man iÅŸleniyor...")
        all_chunks = []
        
        for doc in tqdm(documents, desc="DokÃ¼manlar iÅŸleniyor"):
            chunks = self.process_document(doc)
            all_chunks.extend(chunks)
        
        print(f"âœ… Toplam {len(all_chunks)} chunk oluÅŸturuldu")
        
        # Ä°statistikler
        kt_chunks = len([c for c in all_chunks if c['metadata']['document_type'] == 'KT'])
        kub_chunks = len([c for c in all_chunks if c['metadata']['document_type'] == 'KUB'])
        
        print(f"  ğŸ“‹ KT chunks: {kt_chunks}")
        print(f"  ğŸ“‹ KUB chunks: {kub_chunks}")
        
        # Embedding ve kaydetme
        print(f"\nğŸ¤– Embedding ve ChromaDB'ye kaydetme...")
        self.batch_embed_and_store(all_chunks, batch_size=100)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        # Final istatistikler
        collection_count = self.collection.count()
        
        print(f"\nğŸ¯ FULL SCALE VECTOR DATABASE TAMAMLANDI!")
        print(f"  ğŸ“Š Toplam chunks: {collection_count:,}")
        print(f"  â±ï¸ Ä°ÅŸlem sÃ¼resi: {processing_time/60:.1f} dakika")
        print(f"  âš¡ HÄ±z: {collection_count/(processing_time/60):.0f} chunk/dakika")
        print(f"  ğŸ’¾ Database boyutu: ~{collection_count * 384 * 4 / (1024**3):.2f} GB")
        
        # Test sorgusu
        self.test_search()
        
        return {
            'total_chunks': collection_count,
            'kt_chunks': kt_chunks,
            'kub_chunks': kub_chunks,
            'processing_time_minutes': processing_time / 60,
            'chunks_per_minute': collection_count / (processing_time / 60)
        }
    
    def test_search(self):
        """Vector database'i test et"""
        
        print(f"\nğŸ” VECTOR DATABASE TESTÄ°:")
        
        test_queries = [
            "paracetamol yan etki",
            "aspirin doz",
            "antibiyotik kullanÄ±m",
            "aÄŸrÄ± kesici",
            "ateÅŸ dÃ¼ÅŸÃ¼rÃ¼cÃ¼"
        ]
        
        for query in test_queries:
            results = self.collection.query(
                query_texts=[query],
                n_results=3
            )
            
            if results['documents'] and results['documents'][0]:
                doc = results['documents'][0][0]
                metadata = results['metadatas'][0][0]
                distance = results['distances'][0][0]
                
                print(f"  Query: '{query}'")
                print(f"    âœ… En yakÄ±n: {metadata['drug_name']} ({metadata['document_type']})")
                print(f"    ğŸ“Š Distance: {distance:.3f}")
                print(f"    ğŸ“ Text: {doc[:100]}...")
                print()

def main():
    """Ana fonksiyon"""
    
    # Vector DB oluÅŸtur
    vector_db = PupillicaFullVectorDB()
    
    # Full scale database oluÅŸtur
    stats = vector_db.create_full_vector_database()
    
    return stats

if __name__ == "__main__":
    stats = main()