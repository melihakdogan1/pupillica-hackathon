#!/usr/bin/env python3
"""
PUPILLICA - DOÄRU Ã–lÃ§eklenebilirlik Analizi
GerÃ§ek verilerle 15,000 ilaÃ§ hesaplamasÄ±
"""

def analyze_current_data():
    """Mevcut test verilerini analiz et"""
    
    current_stats = {
        "total_pdfs": 20,           # 10 KT + 10 KUB
        "total_chunks": 816,        # GerÃ§ek chunk sayÄ±sÄ±
        "kt_files": 10,
        "kub_files": 10,
        "kt_chunks": 301,           # GerÃ§ek KT chunk
        "kub_chunks": 515,          # GerÃ§ek KUB chunk
        "avg_chunks_per_pdf": 816 / 20,  # = 40.8 chunk/PDF
        "avg_kt_chunks": 301 / 10,  # = 30.1 chunk/KT
        "avg_kub_chunks": 515 / 10  # = 51.5 chunk/KUB
    }
    
    return current_stats

def calculate_15k_scale():
    """15,000 ilaÃ§ iÃ§in gerÃ§ek hesaplama"""
    
    current = analyze_current_data()
    
    # 15,000 ilaÃ§ = 15,000 KT + 15,000 KUB = 30,000 PDF
    scale_factor = 30000 / current["total_pdfs"]  # 30,000 / 20 = 1,500x
    
    projected = {
        "total_drugs": 15000,
        "total_pdfs": 30000,  # 15k KT + 15k KUB
        "total_chunks": int(current["total_chunks"] * scale_factor),  # 816 Ã— 1,500 = 1,224,000
        "kt_chunks": int(current["kt_chunks"] * (15000 / 10)),       # 301 Ã— 1,500 = 451,500
        "kub_chunks": int(current["kub_chunks"] * (15000 / 10)),     # 515 Ã— 1,500 = 772,500
        "scale_factor": scale_factor
    }
    
    return current, projected

def calculate_resources(chunks):
    """Kaynak gereksinimlerini hesapla"""
    
    embedding_dim = 384  # all-MiniLM-L6-v2
    bytes_per_float = 4
    
    # Memory hesaplamalarÄ±
    embeddings_bytes = chunks * embedding_dim * bytes_per_float
    embeddings_gb = embeddings_bytes / (1024**3)
    
    # Ortalama chunk boyutu (tahmini)
    avg_chunk_size = 400  # karakter
    text_bytes = chunks * avg_chunk_size * 2  # UTF-8 ortalama 2 byte/char
    text_gb = text_bytes / (1024**3)
    
    # ChromaDB overhead (metadata, indexing)
    overhead_factor = 1.5
    total_storage_gb = (embeddings_gb + text_gb) * overhead_factor
    
    # RAM gereksinimi (processing sÄ±rasÄ±nda)
    ram_gb = embeddings_gb * 2  # Ä°ÅŸleme sÄ±rasÄ±nda 2x memory
    
    return {
        "chunks": chunks,
        "embeddings_gb": round(embeddings_gb, 2),
        "text_gb": round(text_gb, 2),
        "total_storage_gb": round(total_storage_gb, 2),
        "ram_needed_gb": round(ram_gb, 2)
    }

def analyze_processing_time():
    """Ä°ÅŸleme sÃ¼resi analizi"""
    
    # Mevcut test: 20 PDF iÃ§in ne kadar sÃ¼rdÃ¼?
    current_pdfs = 20
    estimated_time_per_pdf = 2  # saniye (PDF extraction + embedding)
    
    # 30,000 PDF iÃ§in
    total_pdfs = 30000
    sequential_time_hours = (total_pdfs * estimated_time_per_pdf) / 3600
    
    # Paralel iÅŸleme ile
    cpu_cores = 8  # Ortalama sistem
    parallel_time_hours = sequential_time_hours / cpu_cores
    
    return {
        "sequential_hours": round(sequential_time_hours, 1),
        "parallel_hours": round(parallel_time_hours, 1),
        "recommended_batch_size": 1000  # 1000'lik gruplar halinde
    }

def recommend_optimizations():
    """Optimizasyon Ã¶nerileri"""
    
    return {
        "immediate_actions": [
            "âœ… Mevcut ChromaDB yaklaÅŸÄ±mÄ± 1.2M chunk iÃ§in uygun",
            "âš ï¸ RAM gereksinimi: ~7GB - dikkatli batch processing",
            "ğŸ”§ Chunk boyutunu 500'den 300'e dÃ¼ÅŸÃ¼r",
            "âš¡ Multiprocessing ile PDF extraction hÄ±zlandÄ±r"
        ],
        "architecture_changes": [
            "ğŸ¯ ChromaDB yerine Qdrant'a geÃ§iÅŸ dÃ¼ÅŸÃ¼nÃ¼lebilir",
            "ğŸ“¦ Disk-based processing (batch loading)",
            "ğŸ”„ Incremental indexing (sadece yeni dosyalar)",
            "ğŸ’¾ SSD kullan - disk I/O kritik"
        ],
        "performance_tips": [
            "ğŸš€ GPU kullan embedding generation iÃ§in",
            "ğŸ“Š Progress tracking ve resume capability",
            "ğŸ—œï¸ Text preprocessing - gereksiz content temizle",
            "ğŸ” Smart chunking - sentence boundaries"
        ]
    }

if __name__ == "__main__":
    print("ğŸ” PUPILLICA - GERÃ‡EK Ã–LÃ‡EKLENEBÄ°LÄ°RLÄ°K ANALÄ°ZÄ°")
    print("=" * 70)
    
    current, projected = calculate_15k_scale()
    
    print("ğŸ“Š MEVCUT TEST VERÄ°LERÄ°:")
    print(f"  Total PDFs: {current['total_pdfs']}")
    print(f"  Total Chunks: {current['total_chunks']}")
    print(f"  KT Chunks: {current['kt_chunks']} (avg: {current['avg_kt_chunks']:.1f}/PDF)")
    print(f"  KUB Chunks: {current['kub_chunks']} (avg: {current['avg_kub_chunks']:.1f}/PDF)")
    print(f"  Ortalama: {current['avg_chunks_per_pdf']:.1f} chunk/PDF")
    
    print(f"\nğŸ¯ 15,000 Ä°LAÃ‡ PROJEKSÄ°YONU:")
    print(f"  Total Drugs: {projected['total_drugs']:,}")
    print(f"  Total PDFs: {projected['total_pdfs']:,}")
    print(f"  Total Chunks: {projected['total_chunks']:,}")
    print(f"  Scale Factor: {projected['scale_factor']:,.0f}x")
    
    # Kaynak hesaplamalarÄ±
    current_resources = calculate_resources(current['total_chunks'])
    projected_resources = calculate_resources(projected['total_chunks'])
    
    print(f"\nğŸ’¾ KAYNAK GEREKSÄ°NÄ°MLERÄ°:")
    print(f"  MEVCUT (816 chunks):")
    print(f"    Storage: {current_resources['total_storage_gb']} GB")
    print(f"    RAM: {current_resources['ram_needed_gb']} GB")
    
    print(f"  HEDEF (1.2M chunks):")
    print(f"    Storage: {projected_resources['total_storage_gb']} GB")
    print(f"    RAM: {projected_resources['ram_needed_gb']} GB")
    
    # SÃ¼re analizi
    timing = analyze_processing_time()
    print(f"\nâ±ï¸ Ä°ÅLEME SÃœRESÄ°:")
    print(f"  SÄ±ralÄ± iÅŸlem: {timing['sequential_hours']} saat")
    print(f"  Paralel iÅŸlem: {timing['parallel_hours']} saat")
    print(f"  Ã–nerilen batch: {timing['recommended_batch_size']} PDF/batch")
    
    # Optimizasyonlar
    optimizations = recommend_optimizations()
    
    print(f"\nâœ… HEMEN YAPILACAKLAR:")
    for action in optimizations['immediate_actions']:
        print(f"  {action}")
    
    print(f"\nğŸ—ï¸ MÄ°MARÄ° DEÄÄ°ÅÄ°KLÄ°KLER:")
    for change in optimizations['architecture_changes']:
        print(f"  {change}")
    
    print(f"\nâš¡ PERFORMANS Ä°PUÃ‡LARI:")
    for tip in optimizations['performance_tips']:
        print(f"  {tip}")
    
    print(f"\nğŸ¯ SONUÃ‡:")
    print(f"  âœ… 1.2M chunk iÅŸlenebilir durumda")
    print(f"  âš ï¸ 7GB RAM + 8GB storage gerekli")
    print(f"  ğŸš€ Batch processing ile 2-3 saatte tamamlanÄ±r")
    print(f"  ğŸ’¡ Mevcut ChromaDB yaklaÅŸÄ±mÄ± devam edebilir")